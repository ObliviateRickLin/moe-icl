# MoE-8 with 8-noise levels for fine-grained specialization
# Uses curriculum learning + ST-MoE noisy routing
inherit:
    - ../base_encoder.yaml

model:
    n_dims: 20
    n_embd: 128
    n_layer: 12
    n_head: 4
    # MoE configuration
    use_moe: True
    num_experts: 8
    top_k: 1
    seq_level_routing: False
    aux_loss_coef: 0.01
    router_noise: True
    noise_scale: 1.0

training:
    train_steps: 100001
    tasks:
        - name: noisy_linear_regression
          kwargs: {normalize_w: True, noise_std: 0.05}
        - name: noisy_linear_regression
          kwargs: {normalize_w: True, noise_std: 0.1}
        - name: noisy_linear_regression
          kwargs: {normalize_w: True, noise_std: 0.2}
        - name: noisy_linear_regression
          kwargs: {normalize_w: True, noise_std: 0.3}
        - name: noisy_linear_regression
          kwargs: {normalize_w: True, noise_std: 0.5}
        - name: noisy_linear_regression
          kwargs: {normalize_w: True, noise_std: 0.7}
        - name: noisy_linear_regression
          kwargs: {normalize_w: True, noise_std: 1.0}
        - name: noisy_linear_regression
          kwargs: {normalize_w: True, noise_std: 1.5}
    curriculum:
        dims:
            start: 20
            end: 20
            inc: 1
            interval: 2000
        points:
            start: 21
            end: 21
            inc: 2
            interval: 2000

out_dir: ../results/moe8_8noise

wandb:
    name: "moe8_8noise_noisy_routing"
    project: "moe-icl"
